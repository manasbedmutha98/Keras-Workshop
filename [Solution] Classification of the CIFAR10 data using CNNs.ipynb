{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Classification of the CIFAR10 data using CNNs\n",
    "### Author [Manas Bedmutha](https://github.com/manasbedmutha98)",
    "\n",
    "The [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) has 32 x 32 x 3 (channels) images of 10 different categories which have been numbered as digits from 0 to 9. It is also one of the most common datasets for starting up with deep learning. Its download script comes inbuilt with the keras package.\n",
    "\n",
    "This notebook will walk you through the developing a classification model for the dataset using a <b>Convolutional Neural Network</b> with the help of the <b>Sequenctial API</b> from Keras.\n",
    "\n",
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, MaxPooling2D\n",
    "#from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "\n",
    "from keras.activations import *\n",
    "from keras.optimizers import *\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the dataset\n",
    "\n",
    "CIFAR10 comes as a part of the keras datasets. It contains 50,000 training images while 10,000 test images. Running the next cell will download it to your local systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (10000, 32, 32, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "- Since we are using a convolutional network we do not need to flatten the array into 1D.\n",
    "- Normalize pixel values between -1 and 1 (and Input type should be float)\n",
    "- There are 10 classes so in order to compute the cross entropy loss function we need to one-hot encoded vectors.\n",
    "- The labels refer to the expected classification output for a given image. They are converted from singular to one-hot encoded values from 0 to 9. That is, if a given image corresponds to 5, its encoding will be [0,0,0,0,0,1,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (10000, 32, 32, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "X_test = (X_test.astype(np.float32) - 127.5)/127.5\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "#y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Keras Model\n",
    "\n",
    "Construct a model using the sequential API with the following instructions:\n",
    "\n",
    "- Inputs are normalized using BatchNormaliation followed by a Dropout layer with a rate of 0.3\n",
    "- Then add a 2D convolutional layer with a kernel of 3x3 with 16 filters\n",
    "- Activate the output with a relu using an Activation layer separately\n",
    "- Output from the convolutional layer goes through a MaxPooling layer of size 2\n",
    "- Add another 2D convolutional layer but ensure that the shape remains 'same' as previous layer having 32 filters\n",
    "- Output from the convolutional layer goes through a MaxPooling layer\n",
    "- Then Flatten the output and add a Dropout layer with a rate of 0.3\n",
    "- Connect the output to a Dense layer containing 100 neurons followed by a BatchNormalization \n",
    "- and finally connect to the output layer with an softmax activation function\n",
    "\n",
    "Print the model summary to see the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_11 (Batc (None, 32, 32, 3)         12        \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               313700    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 334,514\n",
      "Trainable params: 334,308\n",
      "Non-trainable params: 206\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(32,32,3)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Activation separate\n",
    "model.add(Conv2D(32,kernel_size=3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(2, padding='valid',strides=2))\n",
    "\n",
    "# Activation in Conv2D definition\n",
    "model.add(Conv2D(64,kernel_size=3, padding='same',activation='sigmoid'))\n",
    "model.add(MaxPooling2D(2,strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model is compiled using an optimizer, a loss function and a metric for performance improvement. \n",
    "- The loss function is used to depict how far is the current model from the ideal answer\n",
    "- The optimizer refers to the method that will be used to minimize the loss\n",
    "- The metrics correspond to how we want to measure the performance of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adagrad(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing\n",
    "\n",
    "- Training is done using the function fit(). We train out network for 5 epochs.\n",
    "- Testing is done using the predict() function. We can also use evaluate() but since we want to later generate a classificiation report, we will use the former\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 102s 2ms/step - loss: 1.6136 - acc: 0.4198\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 124s 2ms/step - loss: 1.3768 - acc: 0.5059\n",
      "Epoch 3/3\n",
      "36096/50000 [====================>.........] - ETA: 43s - loss: 1.2933 - acc: 0.5390"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [8],\n",
       "       [8],\n",
       "       ...,\n",
       "       [5],\n",
       "       [1],\n",
       "       [7]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_check = model.predict(X_test)\n",
    "\n",
    "y_pred = np.array([np.argmax(y_check[j]) for j in range(len(y_check))])\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[516   2 196  30  15  20  12   4 195  10]\n",
      " [124 422  70  33  25  29  18  18 197  64]\n",
      " [ 52   0 643  43  80 103  43  16  20   0]\n",
      " [ 12   1 269 279  70 288  55  10  12   4]\n",
      " [ 27   1 348  56 389  71  55  39  14   0]\n",
      " [  9   0 213 122  50 561  20  17   8   0]\n",
      " [  8   1 252  65  99  50 507   9   8   1]\n",
      " [ 22   0 143  64 111 149  22 477   8   4]\n",
      " [109  14  70  16   8  35   7   6 731   4]\n",
      " [103  83  83  61  24  49  40  44 198 315]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.52      0.52      1000\n",
      "          1       0.81      0.42      0.55      1000\n",
      "          2       0.28      0.64      0.39      1000\n",
      "          3       0.36      0.28      0.32      1000\n",
      "          4       0.45      0.39      0.42      1000\n",
      "          5       0.41      0.56      0.48      1000\n",
      "          6       0.65      0.51      0.57      1000\n",
      "          7       0.75      0.48      0.58      1000\n",
      "          8       0.53      0.73      0.61      1000\n",
      "          9       0.78      0.32      0.45      1000\n",
      "\n",
      "avg / total       0.55      0.48      0.49     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "1. [Keras' official example](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py) on Github\n",
    "2. [Documentation References](https://keras.io/) for more info about every function/layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
